{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53f4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fdb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2b8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    root_dir: str\n",
    "    trained_model_path: str\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    num_epochs: int\n",
    "    patience: int\n",
    "    num_layers_to_unfreeze: int\n",
    "    pretrained_model_path: str\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareCallbacksConfig:\n",
    "    root_dir: Path\n",
    "    tensorboard_root_log_dir: Path\n",
    "    checkpoint_model_filepath: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04e0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "# from cnnClassifier.entity import DataPreprocessingConfig\n",
    "from cnnClassifier.entity.config_entity import PrepareBaseModelConfig\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training_cfg = self.config.training\n",
    "        pretrained_model_path = self.config.prepare_base_model.updated_base_model_path\n",
    "        create_directories([training_cfg.root_dir])\n",
    "        return TrainingConfig(\n",
    "            root_dir=training_cfg.root_dir,\n",
    "            trained_model_path=training_cfg.trained_model_path,\n",
    "            pretrained_model_path=pretrained_model_path,\n",
    "            learning_rate=0.0001,          # you can push this to yaml later\n",
    "            weight_decay=1e-4,\n",
    "            num_epochs=5,\n",
    "            patience=5,\n",
    "            num_layers_to_unfreeze=30\n",
    "        )\n",
    "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
    "        config = self.config.prepare_callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([\n",
    "            Path(model_ckpt_dir),\n",
    "            Path(config.tensorboard_root_log_dir)\n",
    "        ])\n",
    "\n",
    "        prepare_callback_config = PrepareCallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
    "        )\n",
    "\n",
    "        return prepare_callback_config\n",
    "    \n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            updated_base_model_path=Path(config.updated_base_model_path),\n",
    "            model_name=self.params.MODEL_NAME,\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_learning_rate=self.params.LEARNING_RATE,\n",
    "            params_include_top=self.params.INCLUDE_TOP,\n",
    "            params_weights=self.params.WEIGHTS,\n",
    "            params_classes=self.params.CLASSES\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30029e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "class PrepareCallback:\n",
    "    def __init__(self, config: PrepareCallbacksConfig):\n",
    "        self.config = config\n",
    "        self.writer = None\n",
    "        self.ckpt_path = self.config.checkpoint_model_filepath\n",
    "\n",
    "    def _create_tb_writer(self):\n",
    "        \"\"\"Create a TensorBoard writer with timestamped log dir\"\"\"\n",
    "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        tb_running_log_dir = os.path.join(\n",
    "            self.config.tensorboard_root_log_dir,\n",
    "            f\"tb_logs_at_{timestamp}\",\n",
    "        )\n",
    "        os.makedirs(tb_running_log_dir, exist_ok=True)\n",
    "        self.writer = SummaryWriter(log_dir=tb_running_log_dir)\n",
    "        print(f\"[TensorBoard] Logging at: {tb_running_log_dir}\")\n",
    "        return self.writer\n",
    "\n",
    "    def _save_checkpoint(self, model, optimizer, epoch, val_loss):\n",
    "        \"\"\"Save model checkpoint if it's the best so far\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, self.ckpt_path)\n",
    "        print(f\"[Checkpoint] Model saved at {self.ckpt_path}\")\n",
    "\n",
    "    def get_tb_writer(self):\n",
    "        \"\"\"Public method to get TensorBoard writer\"\"\"\n",
    "        if self.writer is None:\n",
    "            self._create_tb_writer()\n",
    "        return self.writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdbfb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, config: TrainingConfig, model: nn.Module,\n",
    "                 train_loader, val_loader,callbacks=None, device=\"cuda\"):\n",
    "        self.config = config\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.callbacks = callbacks if callbacks else []\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load pretrained (updated) weights if available\n",
    "        if self.config.pretrained_model_path and Path(self.config.pretrained_model_path).exists():\n",
    "            print(f\"📥 Loading pretrained weights from {self.config.pretrained_model_path}\")\n",
    "            self.model.load_state_dict(torch.load(self.config.pretrained_model_path, map_location=device))\n",
    "\n",
    "        # Loss, optimizer, scheduler\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    def train(self):\n",
    "        print(f\"\\n🚀 Starting Training for {self.config.num_epochs} epochs\")\n",
    "        print(f\"   - Learning rate: {self.config.learning_rate}\")\n",
    "        print(f\"   - Weight decay: {self.config.weight_decay}\")\n",
    "        print(f\"   - Patience: {self.config.patience}\")\n",
    "        print(f\"   - Unfreezing last {self.config.num_layers_to_unfreeze} layers\")\n",
    "\n",
    "        best_acc = 0.0\n",
    "        patience_counter = 0\n",
    "         # 🔹 If TensorBoard is available\n",
    "        tb_writer = None\n",
    "        for cb in self.callbacks:\n",
    "            if hasattr(cb, \"get_tb_writer\"):\n",
    "                tb_writer = cb.get_tb_writer()\n",
    "\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            # 🔹 Training\n",
    "            self.model.train()\n",
    "            running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_train += (preds == labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "            train_acc = 100 * correct_train / total_train\n",
    "            epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "            \n",
    "\n",
    "            # 🔹 Validation\n",
    "            self.model.eval()\n",
    "            correct_val, total_val = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in self.val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(images)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    correct_val += (preds == labels).sum().item()\n",
    "                    total_val += labels.size(0)\n",
    "\n",
    "            val_acc = 100 * correct_val / total_val\n",
    "\n",
    "            print(f\"📊 Epoch {epoch+1}/{self.config.num_epochs} \"\n",
    "                  f\"| Loss: {epoch_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "            # 🔹 TensorBoard logging\n",
    "            if tb_writer:\n",
    "                tb_writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "                tb_writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "                tb_writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "            # 🔹 Save best model\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                for cb in self.callbacks:\n",
    "                    if hasattr(cb, \"_save_checkpoint\"):\n",
    "                        cb._save_checkpoint(self.model, self.optimizer, epoch, epoch_loss)\n",
    "                torch.save(self.model.state_dict(), self.config.trained_model_path)\n",
    "                print(f\"   💾 Best model updated @ {val_acc:.2f}% \"\n",
    "                      f\"-> saved to {self.config.trained_model_path}\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # 🔹 Early stopping\n",
    "            if patience_counter >= self.config.patience:\n",
    "                print(\"⏹️ Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        print(f\"✅ Training completed! Best Val Accuracy: {best_acc:.2f}%\")\n",
    "        return best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b94816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-02 18:00:28,821: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-02 18:00:28,825: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-02 18:00:28,827: INFO: common: created directory at: artifacts]\n",
      "[2025-10-02 18:00:28,829: INFO: common: created directory at: artifacts/training]\n",
      "[2025-10-02 18:00:28,831: INFO: common: created directory at: artifacts\\prepare_callbacks\\checkpoint_dir]\n",
      "[2025-10-02 18:00:28,833: INFO: common: created directory at: artifacts\\prepare_callbacks\\tensorboard_log_dir]\n",
      "[2025-10-02 18:00:28,836: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-02 18:00:28,839: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-02 18:00:28,841: INFO: common: created directory at: artifacts]\n",
      "[2025-10-02 18:00:28,842: INFO: common: created directory at: artifacts/prepare_base_model]\n",
      "[Init] PrepareBaseModel initialized with model: efficientnet_b0\n",
      "[Pipeline] Preparing base model...\n",
      "[Step 1] Loading base model...\n",
      "[Step 2] Freezing base model layers...\n",
      "[Step 3] Replacing classifier head...\n",
      "[Done] Base model created.\n",
      "[Saved] Model saved at: artifacts\\prepare_base_model\\base_model.pth\n",
      "[Pipeline Done] Base model prepared and saved.\n",
      "[Pipeline] Updating base model...\n",
      "[Step 1] Loading base model...\n",
      "[Step 2] Freezing base model layers...\n",
      "[Step 3] Replacing classifier head...\n",
      "[Done] Base model created.\n",
      "[Step 1] Loaded base model weights.\n",
      "[Step 2] Unfreezing classifier...\n",
      "[Step 3] Unfreezing last 9 backbone layers...\n",
      "[Saved] Model saved at: artifacts\\prepare_base_model\\base_model_updated.pth\n",
      "[Pipeline Done] Updated base model ready for fine-tuning.\n",
      "[Step 1] Loading base model...\n",
      "[Step 2] Freezing base model layers...\n",
      "[Step 3] Replacing classifier head...\n",
      "[Done] Base model created.\n",
      "🚚 Model architecture ready: EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "[2025-10-02 18:00:29,343: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-02 18:00:29,348: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-02 18:00:29,350: INFO: common: created directory at: artifacts]\n",
      "[2025-10-02 18:00:29,361: INFO: stage_03_data_preprocessing: Train loader batches: 56]\n",
      "[2025-10-02 18:00:29,362: INFO: stage_03_data_preprocessing: Validation loader batches: 16]\n",
      "[2025-10-02 18:00:29,363: INFO: stage_03_data_preprocessing: Test loader batches: 8]\n",
      "[2025-10-02 18:00:29,456: INFO: stage_03_data_preprocessing: Sample batch - images shape: torch.Size([32, 3, 224, 224]), labels shape: torch.Size([32])]\n",
      "🚚 Data loaders ready: Train(1770), Val(505), Test(252)\n",
      "📥 Loading pretrained weights from artifacts/prepare_base_model/base_model_updated.pth\n",
      "\n",
      "🚀 Starting Training for 5 epochs\n",
      "   - Learning rate: 0.0001\n",
      "   - Weight decay: 0.0001\n",
      "   - Patience: 5\n",
      "   - Unfreezing last 30 layers\n",
      "[TensorBoard] Logging at: artifacts\\prepare_callbacks\\tensorboard_log_dir\\tb_logs_at_2025-10-02-18-00-29\n",
      "📊 Epoch 1/5 | Loss: 1.5788 | Train Acc: 49.77% | Val Acc: 65.35%\n",
      "[Checkpoint] Model saved at artifacts\\prepare_callbacks\\checkpoint_dir\\model.pth\n",
      "   💾 Best model updated @ 65.35% -> saved to artifacts/training/model.pth\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\el papi\\AppData\\Local\\Temp\\ipykernel_35528\\3065248352.py\", line 30, in <module>\n",
      "    trainer.train()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\el papi\\AppData\\Local\\Temp\\ipykernel_35528\\2268326157.py\", line 58, in train\n",
      "    running_loss += loss.item() * images.size(0)\n",
      "                    ~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2194, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "        etype, value, tb, tb_offset=tb_offset\n",
      "    )\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1179, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, etype, evalue, etb, tb_offset, context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1050, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, etype, evalue, etb, tb_offset, context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 858, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        etype, evalue, etb, context, tb_offset\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 770, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 649, in format_record\n",
      "    frame_info.lines,\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\IPython\\core\\tbtools.py\", line 355, in lines\n",
      "    return self._sd.lines  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ~~~~~~~~~^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ~~~~~~~~~^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ~~~~~~~~~^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\el papi\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "from cnnClassifier.pipeline.stage_03_data_preprocessing import DataPreprocessingTrainingPipeline\n",
    "from cnnClassifier.pipeline.stage_02_prepare_base_model import PrepareBaseModelTrainingPipeline\n",
    "\n",
    "\n",
    " # 1. Load training config\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
    "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    prepare_pipeline = PrepareBaseModelTrainingPipeline()\n",
    "    updated_model, model_architecture = prepare_pipeline.main()\n",
    "    print(f\"🚚 Model architecture ready: {model_architecture}\")\n",
    "    # 2. Get data from preprocessing stage\n",
    "    data_pipeline = DataPreprocessingTrainingPipeline()\n",
    "    train_loader, val_loader, test_loader = data_pipeline.main()\n",
    "    print(f\"🚚 Data loaders ready: Train({len(train_loader.dataset)}), Val({len(val_loader.dataset)}), Test({len(test_loader.dataset)})\")\n",
    "    \n",
    "    # 3. Initialize Training component\n",
    "    trainer = TrainModel(\n",
    "        model=model_architecture,\n",
    "        config=training_config,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        callbacks=[prepare_callbacks]\n",
    "        \n",
    "    )\n",
    "\n",
    "    # 4. Train the model\n",
    "    trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    \n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1800e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-02 22:30:46,830: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-02 22:30:46,835: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-02 22:30:46,836: INFO: common: created directory at: artifacts]\n",
      "[2025-10-02 22:30:46,838: INFO: common: created directory at: artifacts/training]\n",
      "<class '__main__.TrainingConfig'>\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "training_config = config.get_training_config()\n",
    "print(TrainingConfig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
